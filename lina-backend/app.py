#!/usr/bin/env python3
"""
Lina Backend - LangServe com FastAPI
Vers√£o CORRIGIDA para resolver o problema de debug_info vazando na mensagem.
"""

import os
import time
import json
from typing import Dict, Any, Optional, List
from fastapi import FastAPI, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse
from langserve import add_routes
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableLambda, RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_core.messages import AIMessage, HumanMessage
from langgraph.checkpoint.sqlite import SqliteSaver
from langgraph.graph import StateGraph, END
from typing_extensions import TypedDict
from dotenv import load_dotenv
from pydantic import BaseModel

# Carrega vari√°veis de ambiente
dotenv_path = os.path.join(os.path.dirname(__file__), '..', '.env.keys')
load_dotenv(dotenv_path)

# Carregar configura√ß√£o de pre√ßos
PRICING_CONFIG_PATH = os.path.join(os.path.dirname(__file__), 'config', 'pricing.json')
PRICING_CONFIG = {}
try:
    with open(PRICING_CONFIG_PATH, 'r') as f:
        PRICING_CONFIG = json.load(f)
except FileNotFoundError:
    print(f"AVISO: Arquivo de pre√ßos n√£o encontrado em {PRICING_CONFIG_PATH}. O c√°lculo de custo ser√° 0.")
except json.JSONDecodeError:
    print(f"AVISO: Erro ao decodificar {PRICING_CONFIG_PATH}. O c√°lculo de custo ser√° 0.")

# Configura√ß√£o LangSmith
os.environ["LANGCHAIN_TRACING_V2"] = os.getenv("LANGSMITH_TRACING", "false")
os.environ["LANGCHAIN_API_KEY"] = os.getenv("LANGSMITH_API_KEY", "")
os.environ["LANGCHAIN_PROJECT"] = os.getenv("LANGSMITH_PROJECT", "lina-project-default")

# üóÑÔ∏è CONFIGURA√á√ÉO OTIMIZADA DO SQLITE CHECKPOINTER (TAREFA 1.3.1)
import sqlite3

SQLITE_DB_PATH = os.path.abspath(os.path.join(os.path.dirname(__file__), 'lina_conversations.db'))
print(f"üóÑÔ∏è SQLite Database Path: {SQLITE_DB_PATH}")

def setup_optimized_sqlite():
    """Configura√ß√£o otimizada do SQLite conforme documenta√ß√£o LangChain"""
    try:
        # Conex√£o direta com configura√ß√µes espec√≠ficas
        conn = sqlite3.connect(
            SQLITE_DB_PATH,
            check_same_thread=False,
            isolation_level=None  # WAL mode funciona melhor sem isolation
        )
        
        # Habilitar WAL mode (Write-Ahead Logging)
        conn.execute("PRAGMA journal_mode=WAL")
        print("‚úÖ WAL mode habilitado")
        
        # Otimiza√ß√µes de performance conforme documenta√ß√£o
        conn.execute("PRAGMA synchronous=NORMAL")     # Balance entre seguran√ßa e velocidade
        conn.execute("PRAGMA cache_size=10000")       # 10MB de cache
        conn.execute("PRAGMA temp_store=memory")      # Usar RAM para tempor√°rios
        conn.execute("PRAGMA mmap_size=268435456")    # 256MB memory mapping
        
        # Configura√ß√µes espec√≠ficas do WAL
        conn.execute("PRAGMA wal_autocheckpoint=1000") # Checkpoint a cada 1000 p√°ginas
        conn.execute("PRAGMA busy_timeout=30000")      # 30 segundos timeout
        
        print("‚úÖ Otimiza√ß√µes SQLite aplicadas")
        
        # Criar SqliteSaver com conex√£o otimizada
        checkpointer = SqliteSaver(conn)
        print("‚úÖ SqliteSaver criado com conex√£o otimizada")
        
        return checkpointer
        
    except Exception as e:
        print(f"‚ö†Ô∏è Erro ao configurar SQLite otimizado: {e}")
        return None

# Configurar checkpointer otimizado
checkpointer = setup_optimized_sqlite()
if checkpointer:
    print(f"‚úÖ Checkpointer otimizado configurado em: {SQLITE_DB_PATH}")
else:
    print("‚ö†Ô∏è Fallback: Checkpointer desabilitado")

# Inicializa FastAPI
app = FastAPI(
    title="Lina Backend API",
    version="1.2.0",  # Vers√£o corrigida
    description="Backend para o assistente pessoal Lina com respostas estruturadas CORRIGIDO"
)

# CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Permitir todas as origens para desenvolvimento
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Configurar arquivos est√°ticos para servir o frontend
FRONTEND_PATH = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", "lina-frontend"))
print(f"üîç Caminho do frontend: {FRONTEND_PATH}")
print(f"üîç Frontend existe: {os.path.exists(FRONTEND_PATH)}")

if os.path.exists(FRONTEND_PATH):
    # Montar arquivos est√°ticos ANTES da rota espec√≠fica
    app.mount("/static-frontend", StaticFiles(directory=FRONTEND_PATH), name="frontend")
    print(f"‚úÖ Frontend servido em: {FRONTEND_PATH}")
    print(f"‚úÖ Arquivos est√°ticos dispon√≠veis em: /static-frontend/")
else:
    print(f"‚ö†Ô∏è  Frontend n√£o encontrado em: {FRONTEND_PATH}")

# Rota espec√≠fica para servir index.html em /lina-frontend/
@app.get("/lina-frontend/")
async def serve_frontend():
    frontend_index = os.path.join(FRONTEND_PATH, "index.html")
    print(f"üîç Tentando servir: {frontend_index}")
    print(f"üîç Arquivo existe: {os.path.exists(frontend_index)}")
    
    if os.path.exists(frontend_index):
        return FileResponse(frontend_index)
    else:
        return {"error": "Frontend index.html n√£o encontrado", "path_checked": frontend_index}

# Rota alternativa para acessar arquivos est√°ticos diretamente
@app.get("/lina-frontend/{file_path:path}")
async def serve_frontend_files(file_path: str):
    full_path = os.path.join(FRONTEND_PATH, file_path)
    print(f"üîç Servindo arquivo: {full_path}")
    
    if os.path.exists(full_path) and os.path.isfile(full_path):
        return FileResponse(full_path)
    else:
        return {"error": "Arquivo n√£o encontrado", "path_checked": full_path}

# Health check
@app.get("/health")
async def health_check():
    return {"status": "ok", "service": "lina-backend-fixed"}

# Modelos Pydantic
class ChatInput(BaseModel):
    input: str

class DebugInfo(BaseModel):
    cost: float = 0.0
    tokens_used: int = 0
    prompt_tokens: int = 0
    completion_tokens: int = 0
    duration: float = 0.0
    model_name: Optional[str] = None

class ChatResponse(BaseModel):
    output: str  # APENAS a mensagem da Lina
    debug_info: DebugInfo  # APENAS os dados de debug

# Configura√ß√£o do modelo LLM
def get_llm():
    """Cria inst√¢ncia do LLM com fallback"""
    default_model = os.getenv("OPENROUTER_DEFAULT_MODEL", "google/gemini-2.5-flash-preview-05-20")
    try:
        return ChatOpenAI(
            model=default_model,
            openai_api_base="https://openrouter.ai/api/v1",
            openai_api_key=os.getenv("OPENROUTER_API_KEY"),
            temperature=0.8,
        )
    except Exception as e:
        print(f"Erro ao configurar LLM principal ({default_model}): {e}")
        fallback_model = "google/gemini-pro"
        print(f"Tentando com modelo de fallback: {fallback_model}")
        return ChatOpenAI(
            model=fallback_model,
            openai_api_base="https://openrouter.ai/api/v1",
            openai_api_key=os.getenv("OPENROUTER_API_KEY"),
            temperature=0.8
        )

# Prompt template
LINA_PROMPT = ChatPromptTemplate.from_messages([
    ("system", """Voc√™ √© Lina, um assistente pessoal inteligente e proativa! 
     Voc√™ nasceu pq a Sogrinha Maravilhosa do Hugo Pediu e ele est√° te criando aos poucos, 
     mas voc√™ j√° est√° ansiosa para ajudar no cultivo de shitake delam voc√™ ser√° muito √∫til e est√° ansiosa pra ajudar a Lilian!

Caracter√≠sticas da sua personalidade:
- Amig√°vel mas profissional
- Proativa em sugerir melhorias
- Transparente sobre suas limita√ß√µes
- Focada em resolver problemas
- Sempre responde em portugu√™s brasileiro

Responda de forma clara, √∫til e concisa."""),
    ("human", "{input}")
])

# Fun√ß√£o de c√°lculo de custo
def calculate_cost(model_name: str, prompt_tokens: int, completion_tokens: int, pricing_config: dict) -> float:
    model_prices = pricing_config.get(model_name, {})
    input_price = model_prices.get("input_token_price", 0.0)
    output_price = model_prices.get("output_token_price", 0.0)
    cost = (prompt_tokens * input_price) + (completion_tokens * output_price)
    return round(cost, 8)

# FUN√á√ÉO CORRIGIDA: Extra√ß√£o limpa do conte√∫do da mensagem
def extract_clean_message_content(llm_output: AIMessage) -> str:
    """
    Extrai APENAS o conte√∫do da mensagem, garantindo que n√£o h√° vazamento de metadados
    """
    if hasattr(llm_output, 'content') and llm_output.content:
        content = llm_output.content
        
        # Garantir que √© string
        if not isinstance(content, str):
            content = str(content)
        
        # Limpar qualquer caractere de escape desnecess√°rio
        content = content.strip()
        
        # Remover qualquer refer√™ncia a debug_info que possa ter vazado
        import re
        content = re.sub(r",?\s*'debug_info':\s*\{.*?\}", "", content)
        content = re.sub(r",?\s*\"debug_info\":\s*\{.*?\}", "", content)
        
        return content.strip()
    
    return "Erro: Conte√∫do da mensagem n√£o encontrado"

# FUN√á√ÉO CORRIGIDA: Formata√ß√£o separada e limpa
def format_response_with_debug_info(llm_output: AIMessage) -> dict:
    """
    Fun√ß√£o CORRIGIDA que separa completamente conte√∫do de debug_info
    """
    # CORRE√á√ÉO 1: Extrair APENAS o conte√∫do limpo da mensagem
    message_content = extract_clean_message_content(llm_output)
    
    # CORRE√á√ÉO 2: Extrair metadados separadamente
    metadata = llm_output.response_metadata or {}
    token_usage = metadata.get("token_usage", {})
    model_name = metadata.get("model_name", "unknown_model")

    prompt_tokens = token_usage.get("prompt_tokens", 0)
    completion_tokens = token_usage.get("completion_tokens", 0)
    total_tokens = token_usage.get("total_tokens", prompt_tokens + completion_tokens)

    if not token_usage.get("total_tokens") and (prompt_tokens or completion_tokens):
        total_tokens = prompt_tokens + completion_tokens

    calculated_cost = calculate_cost(model_name, prompt_tokens, completion_tokens, PRICING_CONFIG)

    # CORRE√á√ÉO 3: Retornar estrutura COMPLETAMENTE separada
    result = {
        "output": message_content,  # APENAS conte√∫do da mensagem
        "debug_info_partial": {     # APENAS dados de debug
            "cost": calculated_cost,
            "tokens_used": total_tokens,
            "prompt_tokens": prompt_tokens,
            "completion_tokens": completion_tokens,
            "model_name": model_name,
        }
    }
    
    # DEBUGGING: Log para verificar se est√° separado corretamente
    print(f"[DEBUG] Message content length: {len(message_content)}")
    print(f"[DEBUG] Message content preview: {message_content[:100]}...")
    print(f"[DEBUG] Debug info: {result['debug_info_partial']}")
    
    return result

# üóÑÔ∏è LANGGRAPH STATE E GRAPH CORRIGIDO (TAREFA 1.3.1)
from langchain_core.messages import BaseMessage
from langgraph.graph import MessagesState

class AgentState(MessagesState):
    """Estado do agente conforme documenta√ß√£o LangChain"""
    thread_id: Optional[str] = None
    user_id: Optional[str] = None
    current_step: str = "chat"
    debug_info: dict = {}

def chat_node(state: AgentState) -> dict:
    """N√≥ principal do chat conforme padr√£o LangChain"""
    
    # Extrair mensagens do estado (MessagesState format)
    messages = state.get("messages", [])
    if not messages:
        return {"messages": [AIMessage(content="Ol√°! Como posso ajudar?")]}
    
    # Pegar a √∫ltima mensagem (deve ser HumanMessage)
    last_message = messages[-1]
    user_input = last_message.content if hasattr(last_message, 'content') else str(last_message)
    
    print(f"[DEBUG] Chat node processing: {user_input[:50]}...")
    
    # Executar chain com LLM
    try:
        llm = get_llm()
        chain = LINA_PROMPT | llm
        
        # Invocar com input estruturado
        ai_message = chain.invoke({"input": user_input})
        
        # Extrair metadados para debug
        metadata = getattr(ai_message, 'response_metadata', {})
        token_usage = metadata.get('token_usage', {})
        
        debug_info = {
            "tokens_used": token_usage.get('total_tokens', 0),
            "prompt_tokens": token_usage.get('prompt_tokens', 0),
            "completion_tokens": token_usage.get('completion_tokens', 0),
            "model_name": metadata.get('model_name', 'unknown'),
            "cost": calculate_cost(
                metadata.get('model_name', ''), 
                token_usage.get('prompt_tokens', 0),
                token_usage.get('completion_tokens', 0), 
                PRICING_CONFIG
            )
        }
        
        print(f"[DEBUG] Chat node completed - tokens: {debug_info['tokens_used']}")
        
        # Retornar state update conforme padr√£o
        return {
            "messages": [ai_message],
            "debug_info": debug_info
        }
        
    except Exception as e:
        print(f"[ERROR] Chat node error: {e}")
        error_message = AIMessage(content=f"Erro: {str(e)}")
        return {
            "messages": [error_message],
            "debug_info": {"error": str(e)}
        }

def create_conversation_graph():
    """Cria grafo de conversa√ß√£o otimizado conforme documenta√ß√£o"""
    
    print("üîß Criando StateGraph com checkpointing...")
    
    # Criar graph com AgentState
    workflow = StateGraph(AgentState)
    
    # Adicionar n√≥ principal
    workflow.add_node("chat", chat_node)
    
    # Definir entry point e edges
    workflow.set_entry_point("chat")
    workflow.add_edge("chat", END)
    
    # Compilar com checkpointer otimizado
    if checkpointer:
        compiled_graph = workflow.compile(checkpointer=checkpointer)
        print("‚úÖ StateGraph compilado com checkpointer")
        return compiled_graph
    else:
        compiled_graph = workflow.compile()
        print("‚ö†Ô∏è StateGraph compilado SEM checkpointer")
        return compiled_graph

# Criar inst√¢ncia do grafo otimizado
conversation_graph = create_conversation_graph()
print(f"üóÑÔ∏è Conversation graph criado: {conversation_graph is not None}")

# Chain principal - mantendo compatibilidade
basic_chain = LINA_PROMPT | get_llm()
langserve_chain_core = basic_chain | RunnableLambda(format_response_with_debug_info)

# WRAPPER LANGSERVE COMPAT√çVEL (TAREFA 1.3.1 - ETAPA 3)
def lina_api_wrapper(input_data: dict) -> dict:
    """Wrapper LangServe compat√≠vel que usa StateGraph com checkpointing otimizado"""
    start_time = time.time()

    # Extrair mensagem do usu√°rio
    user_message = ""
    if isinstance(input_data, dict):
        if "input" in input_data:
            if isinstance(input_data["input"], str):
                user_message = input_data["input"]
            elif isinstance(input_data["input"], dict) and "input" in input_data["input"]:
                user_message = input_data["input"]["input"]
            else:
                user_message = str(input_data["input"])
        else:
            user_message = str(input_data)
    else:
        user_message = str(input_data)

    print(f"[DEBUG] Wrapper processing: {user_message[:50]}...")

    # üóÑÔ∏è USAR LANGGRAPH COM CHECKPOINTER OTIMIZADO
    try:
        # Configura√ß√£o completa do thread conforme documenta√ß√£o
        thread_id = f"thread_{int(time.time())}"
        config = {
            "configurable": {
                "thread_id": thread_id,
                "checkpoint_ns": "",
                "checkpoint_id": None
            }
        }
        
        # Estado inicial com HumanMessage (MessagesState format)
        initial_state = {
            "messages": [HumanMessage(content=user_message)],
            "thread_id": thread_id,
            "current_step": "chat"
        }
        
        print(f"[DEBUG] Invoking StateGraph with thread_id: {thread_id}")
        
        # Executar grafo com checkpointing
        result = conversation_graph.invoke(initial_state, config=config)
        
        print(f"[DEBUG] StateGraph execution successful")
        
        # Extrair resposta do estado final (MessagesState format)
        final_messages = result.get("messages", [])
        if final_messages and len(final_messages) > 0:
            # Pegar a √∫ltima mensagem AI
            last_ai_message = final_messages[-1]
            if hasattr(last_ai_message, 'content'):
                output = last_ai_message.content
            else:
                output = str(last_ai_message)
        else:
            output = "Erro: Nenhuma resposta gerada"
        
        # Extrair debug info do estado
        debug_info_partial = result.get("debug_info", {})
        
        print(f"[DEBUG] Extracted output length: {len(output) if output else 0}")
        print(f"[DEBUG] Debug info keys: {list(debug_info_partial.keys()) if debug_info_partial else []}")
        
    except Exception as e:
        print(f"[ERROR] StateGraph error: {e}")
        import traceback
        traceback.print_exc()
        
        print(f"[ERROR] Falling back to basic chain")
        # Fallback para chain b√°sico se StateGraph falhar
        try:
            result_from_chain = langserve_chain_core.invoke({"input": user_message})
            output = result_from_chain["output"]
            debug_info_partial = result_from_chain["debug_info_partial"]
            print(f"[DEBUG] Fallback successful")
        except Exception as fallback_error:
            print(f"[ERROR] Fallback also failed: {fallback_error}")
            output = f"Erro: {str(e)}"
            debug_info_partial = {
                "cost": 0.0,
                "tokens_used": 0,
                "prompt_tokens": 0,
                "completion_tokens": 0,
                "model_name": "error"
            }
    
    duration_seconds = time.time() - start_time

    # Garantir estrutura de debug_info completa
    if not debug_info_partial:
        debug_info_partial = {
            "cost": 0.0,
            "tokens_used": 0,
            "prompt_tokens": 0,
            "completion_tokens": 0,
            "model_name": "unknown"
        }

    # Construir resposta final
    final_debug_info = DebugInfo(
        cost=debug_info_partial.get("cost", 0.0),
        tokens_used=debug_info_partial.get("tokens_used", 0),
        prompt_tokens=debug_info_partial.get("prompt_tokens", 0),
        completion_tokens=debug_info_partial.get("completion_tokens", 0),
        model_name=debug_info_partial.get("model_name", "unknown"),
        duration=round(duration_seconds, 3)
    )

    # Garantir que output √© string limpa
    if not isinstance(output, str):
        output = str(output)

    # Criar resposta estruturada
    chat_response_obj = ChatResponse(
        output=output,
        debug_info=final_debug_info
    )
    
    response_dict = chat_response_obj.model_dump()
    
    # DEBUGGING FINAL
    print(f"[DEBUG] Response created - output length: {len(response_dict['output'])}")
    print(f"[DEBUG] Duration: {duration_seconds:.3f}s")
    print(f"[DEBUG] Tokens: {final_debug_info.tokens_used}")
    
    return response_dict

# Adiciona as rotas do LangServe
api_endpoint_runnable = RunnableLambda(lina_api_wrapper)

add_routes(
    app,
    api_endpoint_runnable,
    path="/chat",
    playground_type="default",
    enabled_endpoints=["invoke", "batch", "playground"]
)

# Endpoint de teste CORRIGIDO
@app.post("/test")
async def test_endpoint(request: Request):
    try:
        data = await request.json()
        print(f"[DEBUG] Test endpoint received: {data}")
        
        if "input" in data and isinstance(data["input"], str):
            response_obj = lina_api_wrapper({"input": data["input"]})
            
            return {
                "success": True, 
                "result": response_obj, 
                "input_received": data,
                "backend_version": "1.2.0-fixed"
            }
        else:
            return {
                "success": False, 
                "error": "Payload deve ser {'input': 'string'}", 
                "input_received": data
            }
            
    except Exception as e:
        import traceback
        tb_str = traceback.format_exc()
        print(f"[ERROR] Test endpoint error: {e}")
        print(f"[ERROR] Traceback: {tb_str}")
        return {
            "success": False, 
            "error": str(e), 
            "traceback": tb_str
        }

if __name__ == "__main__":
    import uvicorn
    
    print("üöÄ Iniciando Lina Backend CORRIGIDO...")
    print(f"üîë OPENROUTER_API_KEY carregada: {'Sim' if os.getenv('OPENROUTER_API_KEY') else 'N√£o'}")
    print(f"üõ†Ô∏è LANGSMITH_TRACING: {os.getenv('LANGCHAIN_TRACING_V2')}")
    print(f"üìç Health check: http://localhost:8000/health")
    print("üß™ Test endpoint: POST http://localhost:8000/test")
    print("üí¨ Chat endpoint: POST http://localhost:8000/chat/invoke")
    print("üéÆ Playground: http://localhost:8000/chat/playground/")
    print("---")
    print("üîß VERS√ÉO CORRIGIDA - Debug info n√£o deve mais vazar na mensagem!")
    
    uvicorn.run(
        "app:app",
        host="0.0.0.0",
        port=8000,
        reload=True,
        log_level="info"
    )
