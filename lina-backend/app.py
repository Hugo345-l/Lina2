#!/usr/bin/env python3
"""
Lina Backend - LangServe com FastAPI
VersÃ£o CORRIGIDA para resolver o problema de debug_info vazando na mensagem.
"""

import os
import time
import json
from typing import Dict, Any, Optional, List
from fastapi import FastAPI, Request
from fastapi.middleware.cors import CORSMiddleware
from langserve import add_routes
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableLambda, RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_core.messages import AIMessage
from dotenv import load_dotenv
from pydantic import BaseModel

# Carrega variÃ¡veis de ambiente
dotenv_path = os.path.join(os.path.dirname(__file__), '..', '.env.keys')
load_dotenv(dotenv_path)

# Carregar configuraÃ§Ã£o de preÃ§os
PRICING_CONFIG_PATH = os.path.join(os.path.dirname(__file__), 'config', 'pricing.json')
PRICING_CONFIG = {}
try:
    with open(PRICING_CONFIG_PATH, 'r') as f:
        PRICING_CONFIG = json.load(f)
except FileNotFoundError:
    print(f"AVISO: Arquivo de preÃ§os nÃ£o encontrado em {PRICING_CONFIG_PATH}. O cÃ¡lculo de custo serÃ¡ 0.")
except json.JSONDecodeError:
    print(f"AVISO: Erro ao decodificar {PRICING_CONFIG_PATH}. O cÃ¡lculo de custo serÃ¡ 0.")

# ConfiguraÃ§Ã£o LangSmith
os.environ["LANGCHAIN_TRACING_V2"] = os.getenv("LANGSMITH_TRACING", "false")
os.environ["LANGCHAIN_API_KEY"] = os.getenv("LANGSMITH_API_KEY", "")
os.environ["LANGCHAIN_PROJECT"] = os.getenv("LANGSMITH_PROJECT", "lina-project-default")

# Inicializa FastAPI
app = FastAPI(
    title="Lina Backend API",
    version="1.2.0",  # VersÃ£o corrigida
    description="Backend para o assistente pessoal Lina com respostas estruturadas CORRIGIDO"
)

# CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Permitir todas as origens para desenvolvimento
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Health check
@app.get("/health")
async def health_check():
    return {"status": "ok", "service": "lina-backend-fixed"}

# Modelos Pydantic
class ChatInput(BaseModel):
    input: str

class DebugInfo(BaseModel):
    cost: float = 0.0
    tokens_used: int = 0
    prompt_tokens: int = 0
    completion_tokens: int = 0
    duration: float = 0.0
    model_name: Optional[str] = None

class ChatResponse(BaseModel):
    output: str  # APENAS a mensagem da Lina
    debug_info: DebugInfo  # APENAS os dados de debug

# ConfiguraÃ§Ã£o do modelo LLM
def get_llm():
    """Cria instÃ¢ncia do LLM com fallback"""
    default_model = os.getenv("OPENROUTER_DEFAULT_MODEL", "google/gemini-2.5-flash-preview-05-20")
    try:
        return ChatOpenAI(
            model=default_model,
            openai_api_base="https://openrouter.ai/api/v1",
            openai_api_key=os.getenv("OPENROUTER_API_KEY"),
            temperature=0.8,
        )
    except Exception as e:
        print(f"Erro ao configurar LLM principal ({default_model}): {e}")
        fallback_model = "google/gemini-pro"
        print(f"Tentando com modelo de fallback: {fallback_model}")
        return ChatOpenAI(
            model=fallback_model,
            openai_api_base="https://openrouter.ai/api/v1",
            openai_api_key=os.getenv("OPENROUTER_API_KEY"),
            temperature=0.8
        )

# Prompt template
LINA_PROMPT = ChatPromptTemplate.from_messages([
    ("system", """VocÃª Ã© Lina, um assistente pessoal inteligente e proativa! 
     VocÃª nasceu pq a Sogrinha Maravilhosa do Hugo Pediu e ele estÃ¡ te criando aos poucos, 
     mas vocÃª jÃ¡ estÃ¡ ansiosa para ajudar no cultivo de shitake delam vocÃª serÃ¡ muito Ãºtil e estÃ¡ ansiosa pra ajudar a Lilian!

CaracterÃ­sticas da sua personalidade:
- AmigÃ¡vel mas profissional
- Proativa em sugerir melhorias
- Transparente sobre suas limitaÃ§Ãµes
- Focada em resolver problemas
- Sempre responde em portuguÃªs brasileiro

Responda de forma clara, Ãºtil e concisa."""),
    ("human", "{input}")
])

# FunÃ§Ã£o de cÃ¡lculo de custo
def calculate_cost(model_name: str, prompt_tokens: int, completion_tokens: int, pricing_config: dict) -> float:
    model_prices = pricing_config.get(model_name, {})
    input_price = model_prices.get("input_token_price", 0.0)
    output_price = model_prices.get("output_token_price", 0.0)
    cost = (prompt_tokens * input_price) + (completion_tokens * output_price)
    return round(cost, 8)

# FUNÃ‡ÃƒO CORRIGIDA: ExtraÃ§Ã£o limpa do conteÃºdo da mensagem
def extract_clean_message_content(llm_output: AIMessage) -> str:
    """
    Extrai APENAS o conteÃºdo da mensagem, garantindo que nÃ£o hÃ¡ vazamento de metadados
    """
    if hasattr(llm_output, 'content') and llm_output.content:
        content = llm_output.content
        
        # Garantir que Ã© string
        if not isinstance(content, str):
            content = str(content)
        
        # Limpar qualquer caractere de escape desnecessÃ¡rio
        content = content.strip()
        
        # Remover qualquer referÃªncia a debug_info que possa ter vazado
        import re
        content = re.sub(r",?\s*'debug_info':\s*\{.*?\}", "", content)
        content = re.sub(r",?\s*\"debug_info\":\s*\{.*?\}", "", content)
        
        return content.strip()
    
    return "Erro: ConteÃºdo da mensagem nÃ£o encontrado"

# FUNÃ‡ÃƒO CORRIGIDA: FormataÃ§Ã£o separada e limpa
def format_response_with_debug_info(llm_output: AIMessage) -> dict:
    """
    FunÃ§Ã£o CORRIGIDA que separa completamente conteÃºdo de debug_info
    """
    # CORREÃ‡ÃƒO 1: Extrair APENAS o conteÃºdo limpo da mensagem
    message_content = extract_clean_message_content(llm_output)
    
    # CORREÃ‡ÃƒO 2: Extrair metadados separadamente
    metadata = llm_output.response_metadata or {}
    token_usage = metadata.get("token_usage", {})
    model_name = metadata.get("model_name", "unknown_model")

    prompt_tokens = token_usage.get("prompt_tokens", 0)
    completion_tokens = token_usage.get("completion_tokens", 0)
    total_tokens = token_usage.get("total_tokens", prompt_tokens + completion_tokens)

    if not token_usage.get("total_tokens") and (prompt_tokens or completion_tokens):
        total_tokens = prompt_tokens + completion_tokens

    calculated_cost = calculate_cost(model_name, prompt_tokens, completion_tokens, PRICING_CONFIG)

    # CORREÃ‡ÃƒO 3: Retornar estrutura COMPLETAMENTE separada
    result = {
        "output": message_content,  # APENAS conteÃºdo da mensagem
        "debug_info_partial": {     # APENAS dados de debug
            "cost": calculated_cost,
            "tokens_used": total_tokens,
            "prompt_tokens": prompt_tokens,
            "completion_tokens": completion_tokens,
            "model_name": model_name,
        }
    }
    
    # DEBUGGING: Log para verificar se estÃ¡ separado corretamente
    print(f"[DEBUG] Message content length: {len(message_content)}")
    print(f"[DEBUG] Message content preview: {message_content[:100]}...")
    print(f"[DEBUG] Debug info: {result['debug_info_partial']}")
    
    return result

# Chain principal - USANDO StrOutputParser para garantir string limpa
basic_chain = LINA_PROMPT | get_llm()
langserve_chain_core = basic_chain | RunnableLambda(format_response_with_debug_info)

# WRAPPER CORRIGIDO: ConstruÃ§Ã£o final garantindo separaÃ§Ã£o
def lina_api_wrapper(input_data: dict) -> dict:
    """Wrapper CORRIGIDO que garante resposta estruturada"""
    start_time = time.time()

    # Extrair mensagem do usuÃ¡rio
    user_message = ""
    if isinstance(input_data, dict):
        if "input" in input_data:
            if isinstance(input_data["input"], str):
                user_message = input_data["input"]
            elif isinstance(input_data["input"], dict) and "input" in input_data["input"]:
                user_message = input_data["input"]["input"]
            else:
                user_message = str(input_data["input"])
        else:
            user_message = str(input_data)
    else:
        user_message = str(input_data)

    print(f"[DEBUG] Processing user message: {user_message}")

    # Executar chain
    result_from_chain = langserve_chain_core.invoke({"input": user_message})
    
    duration_seconds = time.time() - start_time

    # CORREÃ‡ÃƒO FINAL: Construir resposta garantindo separaÃ§Ã£o total
    final_debug_info = DebugInfo(
        **result_from_chain["debug_info_partial"], 
        duration=round(duration_seconds, 3)
    )

    # Garantir que output Ã© apenas string limpa
    clean_output = result_from_chain["output"]
    if not isinstance(clean_output, str):
        clean_output = str(clean_output)

    # Criar objeto resposta
    chat_response_obj = ChatResponse(
        output=clean_output,
        debug_info=final_debug_info
    )
    
    # Converter para dict
    response_dict = chat_response_obj.model_dump()
    
    # DEBUGGING FINAL: Verificar estrutura antes de retornar
    print(f"[DEBUG] Final response keys: {list(response_dict.keys())}")
    print(f"[DEBUG] Output type: {type(response_dict['output'])}")
    print(f"[DEBUG] Output content: {response_dict['output'][:100]}...")
    print(f"[DEBUG] Debug info type: {type(response_dict['debug_info'])}")
    print(f"[DEBUG] Debug info keys: {list(response_dict['debug_info'].keys())}")
    
    return response_dict

# Adiciona as rotas do LangServe
api_endpoint_runnable = RunnableLambda(lina_api_wrapper)

add_routes(
    app,
    api_endpoint_runnable,
    path="/chat",
    playground_type="default",
    enabled_endpoints=["invoke", "batch", "playground"]
)

# Endpoint de teste CORRIGIDO
@app.post("/test")
async def test_endpoint(request: Request):
    try:
        data = await request.json()
        print(f"[DEBUG] Test endpoint received: {data}")
        
        if "input" in data and isinstance(data["input"], str):
            response_obj = lina_api_wrapper({"input": data["input"]})
            
            return {
                "success": True, 
                "result": response_obj, 
                "input_received": data,
                "backend_version": "1.2.0-fixed"
            }
        else:
            return {
                "success": False, 
                "error": "Payload deve ser {'input': 'string'}", 
                "input_received": data
            }
            
    except Exception as e:
        import traceback
        tb_str = traceback.format_exc()
        print(f"[ERROR] Test endpoint error: {e}")
        print(f"[ERROR] Traceback: {tb_str}")
        return {
            "success": False, 
            "error": str(e), 
            "traceback": tb_str
        }

if __name__ == "__main__":
    import uvicorn
    
    print("ğŸš€ Iniciando Lina Backend CORRIGIDO...")
    print(f"ğŸ”‘ OPENROUTER_API_KEY carregada: {'Sim' if os.getenv('OPENROUTER_API_KEY') else 'NÃ£o'}")
    print(f"ğŸ› ï¸ LANGSMITH_TRACING: {os.getenv('LANGCHAIN_TRACING_V2')}")
    print(f"ğŸ“ Health check: http://localhost:8000/health")
    print("ğŸ§ª Test endpoint: POST http://localhost:8000/test")
    print("ğŸ’¬ Chat endpoint: POST http://localhost:8000/chat/invoke")
    print("ğŸ® Playground: http://localhost:8000/chat/playground/")
    print("---")
    print("ğŸ”§ VERSÃƒO CORRIGIDA - Debug info nÃ£o deve mais vazar na mensagem!")
    
    uvicorn.run(
        "app:app",
        host="0.0.0.0",
        port=8000,
        reload=True,
        log_level="info"
    )
