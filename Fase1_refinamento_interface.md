# Plano Detalhado: Fase 1 Revisada - MVP Funcional com Interface de Debug

**Data de Revis√£o:** 06/09/2025

## Contexto: Li√ß√µes Aprendidas com Desafios Anteriores no Backend

Antes de detalhar as tarefas desta fase, √© crucial revisitar os aprendizados de um ciclo de desenvolvimento anterior que envolveu o backend LangServe. Compreender esses pontos ajudar√° a evitar a repeti√ß√£o de erros e a guiar as decis√µes t√©cnicas.

### üö® O Problema Inicial
O backend estava funcional, com testes passando e endpoints (`/chat/invoke`, `/chat/stream`) operacionais via LangServe, aceitando o payload no formato `{"input": {"input": "mensagem"}}`. Tentativas de "melhorar" o c√≥digo levaram ao erro 422 (Unprocessable Entity).

### üîç Sequ√™ncia de Erros Observados
1.  **Erro 422 Unprocessable Entity**: `'Input should be a valid dictionary or instance of ChatInput'`.
    *   **Causa**: Incompatibilidade entre o formato de entrada esperado pelo LangServe (ap√≥s modifica√ß√µes na configura√ß√£o do `add_routes` com `input_type=dict`) e o que estava sendo enviado ou como o Pydantic model `ChatInput` estava sendo interpretado.
2.  **Servidor Fechando Imediatamente**: Logs mostravam `INFO: Application startup complete.` seguido por `INFO: Shutting down` e `INFO: Application shutdown complete.`.
    *   **Causa**: Modifica√ß√µes no c√≥digo (provavelmente na tentativa de corrigir o erro 422 ou ao introduzir uma chain LCEL mais complexa) causaram um erro silencioso que impedia o servidor Uvicorn de se manter ativo.
3.  **Connection Refused**: `[WinError 10061] Nenhuma conex√£o p√¥de ser feita porque a m√°quina de destino as recusou`.
    *   **Causa**: Consequ√™ncia direta do Erro 2; o servidor n√£o estava rodando, impedindo que os testes ou a interface se conectassem.

### üîÑ Ciclo Vicioso dos "Fixes"
O processo de tentativa e erro seguiu um padr√£o problem√°tico:
C√≥digo funcionando ‚Üí Tentativa de "melhorar" ‚Üí Erro 422 ‚Üí Mais tentativas de corre√ß√£o (ex: mudar `input_type`) ‚Üí Servidor inst√°vel/fechando ‚Üí Mudan√ßas adicionais na chain ou configura√ß√£o ‚Üí Connection refused ‚Üí Confus√£o e mais "corre√ß√µes" que distanciavam o c√≥digo do estado funcional original.

### üß† An√°lise das Tentativas de Corre√ß√£o
*   **Mudar `input_type=dict` em `add_routes`**:
    *   **Original (funcional)**: `add_routes(app, langserve_chain, path="/chat")` (provavelmente com `with_types` impl√≠cito ou configurado corretamente antes).
    *   **Mudan√ßa (problem√°tica)**: `add_routes(app, chain.with_types(input_type=dict, output_type=ChatResponse))`
    *   **Resultado**: Erro 422, pois o LangServe esperava um dicion√°rio gen√©rico, mas a valida√ß√£o interna ou a chain ainda poderiam estar esperando a estrutura do `ChatInput` ou o formato aninhado.
*   **Implementar LCEL Complexo**: Uma chain como `RunnableLambda(_prepare_input_and_start_time) | RunnablePassthrough.assign(...) | RunnableLambda(_build_final_chat_response)` introduziu complexidade que dificultou o debug e contribuiu para a instabilidade.
*   **Confus√£o com Formatos de Payload**: Testar com `{"input": "texto"}` e `{"input": {"input": "texto"}}` sem clareza de qual era o esperado pela configura√ß√£o atual do LangServe levou a resultados inconsistentes.

### üí° Principais Aprendizados para Esta Fase
1.  **Lei da Complexidade Desnecess√°ria**: Se algo funciona, entenda *profundamente* por que funciona antes de tentar modificar ou "melhorar". Mudan√ßas devem ser incrementais e testadas individualmente.
2.  **LangServe e Suas Conven√ß√µes**:
    *   O formato `{"input": {"input": "mensagem"}}` √© uma caracter√≠stica do LangServe quando se usa um Pydantic model como `input_type` que tem um campo chamado `input`. A camada externa √© o envelope do LangServe, e a interna √© a estrutura do Pydantic.
    *   O playground (`/chat/playground/` ou o tipo especificado em `add_routes`) √© a fonte da verdade para o formato de payload esperado.
    *   Usar `with_types(input_type=ChatInput)` (onde `ChatInput` √© um `BaseModel`) √© geralmente a forma correta de definir tipos de entrada e sa√≠da para valida√ß√£o e gera√ß√£o de esquema OpenAPI.
3.  **Debugging em Cascata √© Perigoso**: Ao encontrar um erro, a primeira a√ß√£o deveria ser reverter para o √∫ltimo estado funcional conhecido. Corrigir um erro introduzindo mais mudan√ßas sem isolar a causa raiz leva a um ciclo de problemas.
4.  **Over-Engineering vs. Simplicidade**: A solu√ß√£o original, provavelmente uma chain mais simples (`PROMPT | LLM | StrOutputParser` ou similar) com `add_routes(app, chain, input_type=ChatInput, output_type=ChatResponse, path="/chat")`, era mais robusta. A tentativa de adicionar camadas de abstra√ß√£o ou l√≥gicas complexas prematuramente foi prejudicial.
5.  **Documenta√ß√£o vs. Realidade do Playground**: Exemplos de documenta√ß√£o podem n√£o se aplicar diretamente. O playground do LangServe mostra o formato exato que a configura√ß√£o *atual* do `add_routes` espera.

### üéØ Estrat√©gias que Funcionaram (e devem ser mantidas)
*   **Volta ao B√°sico**: Restaurar a vers√£o funcional e fazer mudan√ßas incrementais.
*   **Diagn√≥stico Sistem√°tico**: Isolar o problema (servidor, cliente, configura√ß√£o). Usar logs do Uvicorn.
*   **Aceitar as Conven√ß√µes do Framework**: Se o LangServe, com `input_type=ChatInput` (onde `ChatInput` tem um campo `input`), espera `{"input": {"input": "SUA_MENSAGEM"}}`, ent√£o esse √© o formato a ser usado.

### üìö Li√ß√µes para o Futuro (Aplic√°veis a Esta Fase)
*   **Regra de Ouro**: "Se funciona, primeiro entenda por que funciona, depois melhore incrementalmente."
*   **Estrat√©gia de Mudan√ßas**: Backup da vers√£o funcional; uma mudan√ßa por vez; teste imediato; rollback r√°pido se quebrar.
*   **Debugging Efetivo**: Isolar o problema; reproduzir consistentemente; ter um estado funcional conhecido para reverter; fazer a menor mudan√ßa poss√≠vel para testar uma hip√≥tese.

Com esses aprendizados em mente, o plano detalhado abaixo para a Fase 1 Revisada visa construir sobre uma base s√≥lida, priorizando a clareza da configura√ß√£o do LangServe e a integra√ß√£o incremental com a interface Streamlit.

---

**Objetivo Principal:** Entregar uma vers√£o m√≠nima vi√°vel (MVP) da Lina, consistindo em uma interface de chat Streamlit funcional conectada a um backend LangServe. A interface deve incluir um painel de debug b√°sico que exibe m√©tricas chave (custo, tokens, dura√ß√£o) obtidas da resposta estruturada do backend, permitindo testes e observabilidade iniciais.

---

## 1. Backend (`lina-backend/app.py` e novos arquivos)

### 1.1. Arquivo de Configura√ß√£o de Pre√ßos (`pricing.json`)
*   **Status:** Conclu√≠do.

*   **A√ß√£o:** Criar um arquivo `pricing.json` na pasta `lina-backend/config/`.
*   **Conte√∫do:** Este arquivo armazenar√° os custos por token (input e output) para cada modelo LLM utilizado via OpenRouter.
    *   **Formato Exemplo:**
        ```json
        {
          "google/gemini-2.5-flash-preview-05-20": {
            "input_token_price": 0.00000035, 
            "output_token_price": 0.00000070 
          },
          "google/gemini-pro": { 
            "input_token_price": 0.00000050,
            "output_token_price": 0.00000150
          }
        }
        ```
    *   **Importante:** Os pre√ßos devem ser por *token individual*. √â crucial verificar e usar os pre√ßos corretos fornecidos pelo OpenRouter para os modelos exatos em uso.
*   **L√≥gica de Carregamento:** O `lina-backend/app.py` dever√° carregar este arquivo JSON no in√≠cio para que a fun√ß√£o de c√°lculo de custo possa utiliz√°-lo.

### 1.2. Modelos Pydantic para API (em `lina-backend/app.py` ou `lina-backend/models.py`)
*   **Status:** Conclu√≠do.

*   **`ChatInput(BaseModel)`:**
    ```python
    from pydantic import BaseModel
    from typing import Optional, List, Dict, Any # Adicionar List, Dict, Any

    class ChatInput(BaseModel):
        input: str
        # thread_id: Optional[str] = None # Para futuro gerenciamento de sess√£o
    ```
*   **`DebugInfo(BaseModel)`:**
    ```python
    class DebugInfo(BaseModel):
        cost: float = 0.0
        tokens_used: int = 0
        prompt_tokens: int = 0
        completion_tokens: int = 0
        duration: float = 0.0 # Em segundos
        model_name: Optional[str] = None
        # flow_steps: List[Dict[str, Any]] = [] # Para quando LangGraph for implementado
    ```
*   **`ChatResponse(BaseModel)`:**
    ```python
    class ChatResponse(BaseModel):
        output: str  # Mensagem da Lina
        debug_info: DebugInfo
        # thread_id: Optional[str] = None 
    ```

### 1.3. Modifica√ß√£o da Chain Principal do LangServe (em `lina-backend/app.py`)
*   **Status:** Conclu√≠do.

*   **Objetivo:** Alterar a chain para que ela retorne um dicion√°rio com a mensagem e as m√©tricas parciais, em vez de apenas uma string.
*   **Nova L√≥gica da Chain:** `LINA_PROMPT | get_llm() | RunnableLambda(format_response_with_debug_info)`
*   **Fun√ß√£o `format_response_with_debug_info(llm_output: AIMessage) -> dict`:**
    *   Recebe o `AIMessage` do LLM.
    *   Extrai `message_content = llm_output.content`.
    *   Extrai metadados do LLM: `token_usage` (prompt, completion, total tokens) e `model_name` de `llm_output.response_metadata`.
        *   Garantir que `total_tokens` seja a soma de `prompt_tokens` e `completion_tokens` se n√£o vier explicitamente.
    *   Chama a fun√ß√£o `calculate_cost(model_name, prompt_tokens, completion_tokens, pricing_config)` para obter o custo.
        ```python
        # Fun√ß√£o de c√°lculo de custo (a ser definida em app.py)
        # PRICING_CONFIG √© o dict carregado do pricing.json
        def calculate_cost(model_name: str, prompt_tokens: int, completion_tokens: int, pricing_config: dict) -> float:
            model_prices = pricing_config.get(model_name, {})
            input_price = model_prices.get("input_token_price", 0.0)
            output_price = model_prices.get("output_token_price", 0.0)
            cost = (prompt_tokens * input_price) + (completion_tokens * output_price)
            return round(cost, 8) # Usar precis√£o adequada
        ```
    *   Retorna um dicion√°rio:
        ```python
        return {
            "output": message_content,
            "debug_info_partial": {
                "cost": calculated_cost,
                "tokens_used": total_tokens,
                "prompt_tokens": prompt_tokens,
                "completion_tokens": completion_tokens,
                "model_name": model_name,
            }
        }
        ```

### 1.4. Wrapper da API: `lina_api_wrapper` (em `lina-backend/app.py`)
*   **Status:** Conclu√≠do. Retorna `dict` via `model_dump()`.

*   **Objetivo:** Envolver a chamada da chain, medir a dura√ß√£o total e formatar a `ChatResponse` final.
*   **L√≥gica:**
    ```python
    # ... (defini√ß√µes de Pydantic, pricing_config, calculate_cost, format_response_with_debug_info)
    # ... (LINA_PROMPT, get_llm())
    # langserve_chain_core = LINA_PROMPT | get_llm() | RunnableLambda(format_response_with_debug_info)

    def lina_api_wrapper(input_data: dict) -> dict: # Retorna dict
        start_time = time.time()
        # ... (extra√ß√£o de user_message de input_data) ...
        result_from_chain = langserve_chain_core.invoke({"input": user_message})
        duration_seconds = time.time() - start_time
        final_debug_info = DebugInfo(
            **result_from_chain["debug_info_partial"], 
            duration=round(duration_seconds, 3)
        )
        chat_response_obj = ChatResponse(
            output=result_from_chain["output"],
            debug_info=final_debug_info
        )
        return chat_response_obj.model_dump() # Retorna dict
    ```

### 1.5. Atualiza√ß√£o do `add_routes` (LangServe em `lina-backend/app.py`)
*   **Status:** Conclu√≠do. `input_type` e `output_type` removidos.

*   **Objetivo:** Configurar o LangServe para usar o novo wrapper e os tipos de dados.
*   **L√≥gica:**
    ```python
    from langserve import add_routes 
    api_endpoint_runnable = RunnableLambda(lina_api_wrapper)
    add_routes(
        app,
        api_endpoint_runnable, 
        path="/chat",
        # input_type e output_type removidos
        playground_type="default",
        enabled_endpoints=["invoke", "batch", "playground"] 
    )
    ```
    *   **Streaming:** Para a Fase 1, o foco √© no endpoint `invoke`. A adapta√ß√£o para streaming (`/chat/stream`) pode ser feita em uma itera√ß√£o seguinte, exigindo um wrapper de streaming que formate os chunks.

### 1.6. Testes do Backend
*   **Status:** Conclu√≠do. Testes em `test_backend.py` para `/chat/invoke` passam, confirmando que o backend retorna o `dict` JSON esperado.

*   Utilizar `lina-backend/test_backend.py` ou ferramentas como Postman/curl.
*   Verificar se `POST /chat/invoke` com `{"input": {"input": "mensagem"}}` retorna a estrutura `ChatResponse` (como `dict`) completa, incluindo `debug_info` com todos os campos esperados.

---

## 2. Frontend (`lina-streamlit-ui/app_st.py`)
*   **Status Geral (10/06/2025):** Implementado, mas com problema cr√≠tico no parsing/exibi√ß√£o da resposta do backend.

### 2.1. Tema Dark B√°sico e CSS Organizado
*   **Status:** Conclu√≠do.

*   Aplicar um tema dark simples via CSS.
*   **Estrat√©gia:** Criar `lina-streamlit-ui/styles.css` e carreg√°-lo, ou definir o CSS como uma string em `styles.py` e usar `st.markdown(CSS_STRING, unsafe_allow_html=True)`.
    *   Focar em cores de fundo s√≥lidas (`#1E1E2E`, `#2D2D44`), texto claro e uma cor de destaque.

### 2.2. Chat com Avatars Simples
*   **Status:** Conclu√≠do.

*   Usar `st.chat_message(name="user", avatar="üë§")` e `st.chat_message(name="Lina", avatar="ü§ñ")`.
*   Estilizar os bal√µes de mensagem com fundos diferentes via CSS, se necess√°rio, para melhor distin√ß√£o.

### 2.3. Debug Panel B√°sico
*   **Status:** Implementado, mas n√£o funcional devido ao problema de parsing da resposta.

*   Implementar com `st.expander("üîç Painel de Debug")`.
*   Dentro do expander, exibir as m√©tricas do `st.session_state.debug_info` (que ser√° o objeto `DebugInfo` da resposta do backend):
    *   `Custo Total: ${debug_data.cost:.6f}`
    *   `Dura√ß√£o: {debug_data.duration:.2f}s`
    *   `Modelo: {debug_data.model_name}`
    *   `Tokens: Total={debug_data.tokens_used}, Prompt={debug_data.prompt_tokens}, Comp.={debug_data.completion_tokens}`
    *   `Fluxo de Execu√ß√£o (Backend):` (Para Fase 1, mostrar um passo √∫nico: "‚úÖ Processamento Principal ({debug_data.duration:.2f}s)")

### 2.4. Input Area Melhorada (Simples)
*   **Status:** Conclu√≠do.

*   Aplicar CSS para `border-radius` e um sutil `box-shadow` ou mudan√ßa de cor da borda no `:focus` do `st.text_input` ou `st.chat_input`.

### 2.5. Processamento da Resposta do Backend
*   **Status:** Implementado com v√°rias tentativas de corre√ß√£o, mas ainda falhando em parsear corretamente a resposta para exibi√ß√£o.

*   A fun√ß√£o que envia a mensagem do usu√°rio ao backend deve:
    *   Esperar uma resposta JSON e desserializ√°-la para um objeto `ChatResponse` (ou um dicion√°rio com a mesma estrutura).
    *   Armazenar `response.debug_info` em `st.session_state.debug_info`.
    *   Adicionar `response.output` ao hist√≥rico de chat.
    *   Chamar `st.rerun()` para atualizar a UI.

### 2.6. Gerenciamento de Estado (`st.session_state`)
*   **Status:** Conclu√≠do.

*   Inicializar no come√ßo do script:
    ```python
    if 'chat_history' not in st.session_state:
        st.session_state.chat_history = [] # Lista de {"role": "user/assistant", "content": "..."}
    if 'debug_info' not in st.session_state:
        st.session_state.debug_info = None # Armazenar√° o objeto DebugInfo
    ```

---

## 3. Graph de Teste (Calculadora - Conceito para Fase 1)
*   **Status:** N√£o implementado. Foco na corre√ß√£o do fluxo principal.

*   **Objetivo:** Validar que o frontend consegue exibir o `debug_info` corretamente.
*   **Implementa√ß√£o Simplificada para Fase 1:**
    *   Criar um endpoint de teste no backend (ex: `POST /calculator/invoke`) que *n√£o* usa LLM.
    *   Este endpoint recebe uma express√£o matem√°tica simples.
    *   Ele calcula o resultado.
    *   Retorna um `ChatResponse` simulado, onde `output` √© o resultado do c√°lculo, e `debug_info` tem `cost=0.0`, `tokens_used=0`, `model_name="calculadora_local"`, e `duration` √© o tempo que levou para calcular.
    *   O frontend pode ter uma forma de interagir com este endpoint para testar o painel de debug.

---

## üèÅ Resultado Esperado da Fase 1 Revisada

*   **Interface Streamlit Funcional:**
    *   Permite enviar mensagens para o backend da Lina.
    *   Exibe a conversa de forma clara.
    *   Apresenta um painel de debug colaps√°vel.
*   **Painel de Debug Informativo:**
    *   Mostra o custo estimado da √∫ltima intera√ß√£o com o LLM.
    *   Mostra o total de tokens, tokens de prompt e tokens de completude.
    *   Mostra a dura√ß√£o total da resposta do backend.
    *   Mostra o nome do modelo LLM utilizado.
    *   (Inicialmente) Mostra um "fluxo" de um √∫nico passo.
*   **Backend LangServe Modificado:**
    *   Retorna respostas no formato `ChatResponse` (como `dict` via `model_dump()`), incluindo o objeto `debug_info`.
    *   Calcula o custo b√°sico com base no `pricing.json`.
    *   Extrai informa√ß√µes de tokens da resposta do LLM.
    *   Mede a dura√ß√£o da execu√ß√£o.
*   **Base S√≥lida:** Uma funda√ß√£o robusta para as pr√≥ximas fases, com observabilidade integrada na ferramenta de teste desde o in√≠cio.

---
## üìù Resumo do Estado Atual e Li√ß√µes Aprendidas (10/06/2025)

Ap√≥s um ciclo de desenvolvimento e depura√ß√£o focado em fazer o backend LangServe (`lina-backend/app.py`) comunicar-se corretamente com o frontend Streamlit (`lina-streamlit-ui/app_st.py`) e exibir informa√ß√µes de debug, o estado √© o seguinte:

**Avan√ßos Conquistados:**
1.  **Backend Robusto:**
    *   O servidor `lina-backend/app.py` est√° funcional e o endpoint `/chat/invoke` retorna um dicion√°rio JSON estruturado corretamente (contendo `output` e `debug_info`), conforme validado pelos testes em `test_backend.py`.
    *   A quest√£o da serializa√ß√£o de objetos Pydantic pelo LangServe foi resolvida utilizando `model_dump()` na fun√ß√£o `lina_api_wrapper` para garantir que um dicion√°rio puro seja retornado.
    *   A configura√ß√£o de `add_routes` foi ajustada para n√£o especificar `input_type` nem `output_type`, permitindo que o LangServe lide melhor com o dicion√°rio retornado.
    *   C√°lculo de custo, extra√ß√£o de tokens e medi√ß√£o de dura√ß√£o est√£o implementados no backend.
2.  **Frontend Conectado:**
    *   A interface `lina-streamlit-ui/app_st.py` consegue enviar requisi√ß√µes para o backend e receber respostas.
    *   Foram feitas v√°rias itera√ß√µes na l√≥gica de tratamento de payload e de resposta no frontend.

**Principal Problema Pendente:**
*   **Parsing/Exibi√ß√£o da Resposta no Streamlit:** Apesar dos avan√ßos e do backend agora enviar um dicion√°rio JSON correto (verificado pelos testes do `test_backend.py`), a interface Streamlit (conforme imagem de 10/06/2025, 01:29 AM) ainda:
    1.  Exibe a resposta completa do backend (o dicion√°rio JSON como uma string literal: `"{'output': '...', 'debug_info': {...}}"`) no bal√£o de chat da Lina, em vez de apenas o valor da chave `'output'`.
    2.  N√£o popula corretamente o "Painel de Debug" com os dados de `debug_info`. Os valores aparecem zerados ou com placeholders como "N/A" ou "Formato Antigo Detectado", indicando que a l√≥gica em `lina-streamlit-ui/app_st.py` n√£o est√° conseguindo extrair e utilizar os dados do dicion√°rio `debug_info` que √© parte da resposta.

**Li√ß√µes Aprendidas Chave (Itera√ß√£o Atual):**
1.  **A Import√¢ncia do `.model_dump()`:** Para intera√ß√µes com LangServe onde se retorna estruturas Pydantic atrav√©s de `RunnableLambda`s, converter explicitamente para `dict` com `.model_dump()` √© crucial para evitar problemas de serializa√ß√£o (onde o LangServe pode acabar tratando o objeto como uma string).
2.  **Flexibilidade no `add_routes`:** Em alguns casos, especialmente com wrappers, remover `input_type` e `output_type` de `add_routes` e deixar o LangServe inferir ou tratar os dados como dicion√°rios pode ser mais robusto.
3.  **Depura√ß√£o Detalhada Frontend-Backend:** O problema atual demonstra que mesmo com um backend enviando dados corretos (verificado por testes diretos da API), a forma como o frontend recebe, parseia e utiliza esses dados √© uma etapa cr√≠tica que tamb√©m precisa de depura√ß√£o cuidadosa. A exibi√ß√£o da string literal no chat e o painel de debug n√£o funcional s√£o sintomas de um problema no parsing do JSON ou na subsequente atribui√ß√£o de valores no c√≥digo Streamlit.

**Pr√≥ximo Passo Cr√≠tico:**
*   **Corrigir o parsing da resposta JSON no `lina-streamlit-ui/app_st.py`:** √â necess√°rio revisar a se√ß√£o `try...except` que processa `response_data_dict = response.json()`. O objetivo √© garantir que:
    *   `assistant_response_content` receba apenas o valor de `response_data_dict['output']`.
    *   `st.session_state.debug_info` seja populado com um objeto `DebugInfo` instanciado a partir do dicion√°rio `response_data_dict['debug_info']`.

Resolver este √∫ltimo ponto de parsing no frontend √© essencial para concluir os objetivos da Fase 1 Revisada e ter uma interface de debug verdadeiramente funcional.
