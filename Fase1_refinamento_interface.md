# Plano Detalhado: Fase 1 Revisada - MVP Funcional com Interface de Debug

**Data de Revis√£o:** 06/09/2025

**Objetivo Principal:** Entregar uma vers√£o m√≠nima vi√°vel (MVP) da Lina, consistindo em uma interface de chat Streamlit funcional conectada a um backend LangServe. A interface deve incluir um painel de debug b√°sico que exibe m√©tricas chave (custo, tokens, dura√ß√£o) obtidas da resposta estruturada do backend, permitindo testes e observabilidade iniciais.

---

## 1. Backend (`lina-backend/app.py` e novos arquivos)

### 1.1. Arquivo de Configura√ß√£o de Pre√ßos (`pricing.json`)

*   **A√ß√£o:** Criar um arquivo `pricing.json` na pasta `lina-backend/config/`.
*   **Conte√∫do:** Este arquivo armazenar√° os custos por token (input e output) para cada modelo LLM utilizado via OpenRouter.
    *   **Formato Exemplo:**
        ```json
        {
          "google/gemini-2.5-flash-preview-05-20": {
            "input_token_price": 0.00000035, 
            "output_token_price": 0.00000070 
          },
          "google/gemini-pro": { 
            "input_token_price": 0.00000050,
            "output_token_price": 0.00000150
          }
        }
        ```
    *   **Importante:** Os pre√ßos devem ser por *token individual*. √â crucial verificar e usar os pre√ßos corretos fornecidos pelo OpenRouter para os modelos exatos em uso.
*   **L√≥gica de Carregamento:** O `lina-backend/app.py` dever√° carregar este arquivo JSON no in√≠cio para que a fun√ß√£o de c√°lculo de custo possa utiliz√°-lo.

### 1.2. Modelos Pydantic para API (em `lina-backend/app.py` ou `lina-backend/models.py`)

*   **`ChatInput(BaseModel)`:**
    ```python
    from pydantic import BaseModel
    from typing import Optional, List, Dict, Any # Adicionar List, Dict, Any

    class ChatInput(BaseModel):
        input: str
        # thread_id: Optional[str] = None # Para futuro gerenciamento de sess√£o
    ```
*   **`DebugInfo(BaseModel)`:**
    ```python
    class DebugInfo(BaseModel):
        cost: float = 0.0
        tokens_used: int = 0
        prompt_tokens: int = 0
        completion_tokens: int = 0
        duration: float = 0.0 # Em segundos
        model_name: Optional[str] = None
        # flow_steps: List[Dict[str, Any]] = [] # Para quando LangGraph for implementado
    ```
*   **`ChatResponse(BaseModel)`:**
    ```python
    class ChatResponse(BaseModel):
        output: str  # Mensagem da Lina
        debug_info: DebugInfo
        # thread_id: Optional[str] = None 
    ```

### 1.3. Modifica√ß√£o da Chain Principal do LangServe (em `lina-backend/app.py`)

*   **Objetivo:** Alterar a chain para que ela retorne um dicion√°rio com a mensagem e as m√©tricas parciais, em vez de apenas uma string.
*   **Nova L√≥gica da Chain:** `LINA_PROMPT | get_llm() | RunnableLambda(format_response_with_debug_info)`
*   **Fun√ß√£o `format_response_with_debug_info(llm_output: AIMessage) -> dict`:**
    *   Recebe o `AIMessage` do LLM.
    *   Extrai `message_content = llm_output.content`.
    *   Extrai metadados do LLM: `token_usage` (prompt, completion, total tokens) e `model_name` de `llm_output.response_metadata`.
        *   Garantir que `total_tokens` seja a soma de `prompt_tokens` e `completion_tokens` se n√£o vier explicitamente.
    *   Chama a fun√ß√£o `calculate_cost(model_name, prompt_tokens, completion_tokens, pricing_config)` para obter o custo.
        ```python
        # Fun√ß√£o de c√°lculo de custo (a ser definida em app.py)
        # PRICING_CONFIG √© o dict carregado do pricing.json
        def calculate_cost(model_name: str, prompt_tokens: int, completion_tokens: int, pricing_config: dict) -> float:
            model_prices = pricing_config.get(model_name, {})
            input_price = model_prices.get("input_token_price", 0.0)
            output_price = model_prices.get("output_token_price", 0.0)
            cost = (prompt_tokens * input_price) + (completion_tokens * output_price)
            return round(cost, 8) # Usar precis√£o adequada
        ```
    *   Retorna um dicion√°rio:
        ```python
        return {
            "output": message_content,
            "debug_info_partial": {
                "cost": calculated_cost,
                "tokens_used": total_tokens,
                "prompt_tokens": prompt_tokens,
                "completion_tokens": completion_tokens,
                "model_name": model_name,
            }
        }
        ```

### 1.4. Wrapper da API: `lina_api_wrapper` (em `lina-backend/app.py`)

*   **Objetivo:** Envolver a chamada da chain, medir a dura√ß√£o total e formatar a `ChatResponse` final.
*   **L√≥gica:**
    ```python
    import time 
    from langchain_core.runnables import RunnableLambda # Adicionar se n√£o estiver
    from langchain_core.messages import AIMessage # Adicionar se n√£o estiver

    # ... (defini√ß√µes de Pydantic, pricing_config, calculate_cost, format_response_with_debug_info)
    # ... (LINA_PROMPT, get_llm())
    # langserve_chain_core = LINA_PROMPT | get_llm() | RunnableLambda(format_response_with_debug_info)


    async def lina_api_wrapper(chat_input: ChatInput) -> ChatResponse:
        start_time = time.time()

        # Supondo que langserve_chain_core est√° definida como acima
        result_from_chain = await langserve_chain_core.ainvoke({"input": chat_input.input})

        duration_seconds = time.time() - start_time

        final_debug_info = DebugInfo(
            **result_from_chain["debug_info_partial"], 
            duration=round(duration_seconds, 3)
        )

        return ChatResponse(
            output=result_from_chain["output"],
            debug_info=final_debug_info
        )
    ```

### 1.5. Atualiza√ß√£o do `add_routes` (LangServe em `lina-backend/app.py`)

*   **Objetivo:** Configurar o LangServe para usar o novo wrapper e os tipos de dados.
*   **L√≥gica:**
    ```python
    from langserve import add_routes 

    # ... (app FastAPI, lina_api_wrapper, ChatInput, ChatResponse)

    add_routes(
        app,
        lina_api_wrapper, 
        path="/chat",
        input_type=ChatInput,    
        output_type=ChatResponse, 
        playground_type="default" 
    )
    ```
    *   **Streaming:** Para a Fase 1, o foco √© no endpoint `invoke`. A adapta√ß√£o para streaming (`/chat/stream`) pode ser feita em uma itera√ß√£o seguinte, exigindo um wrapper de streaming que formate os chunks.

### 1.6. Testes do Backend

*   Utilizar `lina-backend/test_backend.py` ou ferramentas como Postman/curl.
*   Verificar se `POST /chat/invoke` com `{"input": "mensagem"}` retorna a estrutura `ChatResponse` completa, incluindo `debug_info` com todos os campos esperados.

---

## 2. Frontend (`lina-streamlit-ui/app_st.py`)

### 2.1. Tema Dark B√°sico e CSS Organizado

*   Aplicar um tema dark simples via CSS.
*   **Estrat√©gia:** Criar `lina-streamlit-ui/styles.css` e carreg√°-lo, ou definir o CSS como uma string em `styles.py` e usar `st.markdown(CSS_STRING, unsafe_allow_html=True)`.
    *   Focar em cores de fundo s√≥lidas (`#1E1E2E`, `#2D2D44`), texto claro e uma cor de destaque.

### 2.2. Chat com Avatars Simples

*   Usar `st.chat_message(name="user", avatar="üë§")` e `st.chat_message(name="Lina", avatar="ü§ñ")`.
*   Estilizar os bal√µes de mensagem com fundos diferentes via CSS, se necess√°rio, para melhor distin√ß√£o.

### 2.3. Debug Panel B√°sico

*   Implementar com `st.expander("üîç Painel de Debug")`.
*   Dentro do expander, exibir as m√©tricas do `st.session_state.debug_info` (que ser√° o objeto `DebugInfo` da resposta do backend):
    *   `Custo Total: ${debug_data.cost:.6f}`
    *   `Dura√ß√£o: {debug_data.duration:.2f}s`
    *   `Modelo: {debug_data.model_name}`
    *   `Tokens: Total={debug_data.tokens_used}, Prompt={debug_data.prompt_tokens}, Comp.={debug_data.completion_tokens}`
    *   `Fluxo de Execu√ß√£o (Backend):` (Para Fase 1, mostrar um passo √∫nico: "‚úÖ Processamento Principal ({debug_data.duration:.2f}s)")

### 2.4. Input Area Melhorada (Simples)

*   Aplicar CSS para `border-radius` e um sutil `box-shadow` ou mudan√ßa de cor da borda no `:focus` do `st.text_input` ou `st.chat_input`.

### 2.5. Processamento da Resposta do Backend

*   A fun√ß√£o que envia a mensagem do usu√°rio ao backend deve:
    *   Esperar uma resposta JSON e desserializ√°-la para um objeto `ChatResponse` (ou um dicion√°rio com a mesma estrutura).
    *   Armazenar `response.debug_info` em `st.session_state.debug_info`.
    *   Adicionar `response.output` ao hist√≥rico de chat.
    *   Chamar `st.rerun()` para atualizar a UI.

### 2.6. Gerenciamento de Estado (`st.session_state`)

*   Inicializar no come√ßo do script:
    ```python
    if 'chat_history' not in st.session_state:
        st.session_state.chat_history = [] # Lista de {"role": "user/assistant", "content": "..."}
    if 'debug_info' not in st.session_state:
        st.session_state.debug_info = None # Armazenar√° o objeto DebugInfo
    ```

---

## 3. Graph de Teste (Calculadora - Conceito para Fase 1)

*   **Objetivo:** Validar que o frontend consegue exibir o `debug_info` corretamente.
*   **Implementa√ß√£o Simplificada para Fase 1:**
    *   Criar um endpoint de teste no backend (ex: `POST /calculator/invoke`) que *n√£o* usa LLM.
    *   Este endpoint recebe uma express√£o matem√°tica simples.
    *   Ele calcula o resultado.
    *   Retorna um `ChatResponse` simulado, onde `output` √© o resultado do c√°lculo, e `debug_info` tem `cost=0.0`, `tokens_used=0`, `model_name="calculadora_local"`, e `duration` √© o tempo que levou para calcular.
    *   O frontend pode ter uma forma de interagir com este endpoint para testar o painel de debug.

---

## üèÅ Resultado Esperado da Fase 1 Revisada

*   **Interface Streamlit Funcional:**
    *   Permite enviar mensagens para o backend da Lina.
    *   Exibe a conversa de forma clara.
    *   Apresenta um painel de debug colaps√°vel.
*   **Painel de Debug Informativo:**
    *   Mostra o custo estimado da √∫ltima intera√ß√£o com o LLM.
    *   Mostra o total de tokens, tokens de prompt e tokens de completude.
    *   Mostra a dura√ß√£o total da resposta do backend.
    *   Mostra o nome do modelo LLM utilizado.
    *   (Inicialmente) Mostra um "fluxo" de um √∫nico passo.
*   **Backend LangServe Modificado:**
    *   Retorna respostas no formato `ChatResponse`, incluindo o objeto `debug_info`.
    *   Calcula o custo b√°sico com base no `pricing.json`.
    *   Extrai informa√ß√µes de tokens da resposta do LLM.
    *   Mede a dura√ß√£o da execu√ß√£o.
*   **Base S√≥lida:** Uma funda√ß√£o robusta para as pr√≥ximas fases, com observabilidade integrada na ferramenta de teste desde o in√≠cio.

---
