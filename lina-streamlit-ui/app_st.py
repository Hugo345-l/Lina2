import streamlit as st
import requests
import json

# URL do backend LangServe
BACKEND_URL_INVOKE = "http://localhost:8000/chat/invoke"
# BACKEND_URL_STREAM = "http://localhost:8000/chat/stream" # Para implementa√ß√£o futura

st.set_page_config(page_title="Lina Chat", page_icon="ü§ñ")

st.title("ü§ñ Lina - IA da Lilian")

# Inicializa o hist√≥rico da conversa na session_state se n√£o existir
if "messages" not in st.session_state:
    st.session_state.messages = []

# Exibe as mensagens do hist√≥rico
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# Input do usu√°rio
if prompt := st.chat_input("Converse com a Lina..."):
    # Adiciona a mensagem do usu√°rio ao hist√≥rico e exibe
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # Prepara a mensagem para o backend
    payload = {"input": {"input": prompt}} # Formato esperado pelo backend LangServe

    # Envia a mensagem para o backend e obt√©m a resposta
    try:
        response = requests.post(BACKEND_URL_INVOKE, json=payload)
        response.raise_for_status()  # Levanta um erro para respostas HTTP ruins (4xx ou 5xx)
        
        # A resposta do /chat/invoke com StrOutputParser √© diretamente a string da IA
        # Se a chain principal no backend retornar um dicion√°rio com uma chave 'output',
        # como √© comum em algumas configura√ß√µes do LangServe, voc√™ precisaria ajustar aqui.
        # Ex: assistant_response = response.json().get("output", "Erro ao processar resposta.")
        # No nosso caso, com StrOutputParser, a resposta JSON √© algo como:
        # {"output": "Ol√°! Como posso ajudar?", "metadata": {...}}
        # ou apenas a string diretamente se a rota for configurada para output direto.
        # O add_routes com StrOutputParser geralmente resulta em:
        # {"output": "string da IA", "metadata": {"run_id": "..."}}
        
        response_data = response.json()
        assistant_response = response_data.get("output", "Desculpe, n√£o consegui processar sua solicita√ß√£o.")
        
        # Adiciona a resposta da Lina ao hist√≥rico e exibe
        st.session_state.messages.append({"role": "assistant", "content": assistant_response})
        with st.chat_message("assistant"):
            st.markdown(assistant_response)

    except requests.exceptions.RequestException as e:
        st.error(f"Erro de conex√£o com o backend: {e}")
        st.session_state.messages.append({"role": "assistant", "content": f"Erro de conex√£o: {e}"})
    except json.JSONDecodeError:
        st.error("Erro ao decodificar a resposta do backend. Resposta n√£o era um JSON v√°lido.")
        st.session_state.messages.append({"role": "assistant", "content": "Erro: Resposta inv√°lida do backend."})
    except Exception as e:
        st.error(f"Ocorreu um erro inesperado: {e}")
        st.session_state.messages.append({"role": "assistant", "content": f"Erro inesperado: {e}"})
